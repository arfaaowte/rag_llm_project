from pydantic import BaseModel, Field, ValidationError
from typing import Optional
from fastapi import Form, HTTPException

from app.utilities import constants


class QueryParameters(BaseModel):
    """
    This class defines the parameters required for the query to the llm.
    """
    query: str
    index_name: str
    filename: Optional[str] = Field(default=None,)
    llm_type: str
    max_tokens: int = Field(100, ge=1)
    temperature: float = Field(0.3, ge=0, le=1.0)
    top_p: float = Field(0.9, ge=0, le=1.0)
    top_k: int = Field(1, ge=1, le=5)
    disable_public_response: Optional[bool] = Field(True)
    enable_evaluation: bool = Field(False)
    expected_answer: Optional[str] = Field(None)


def parse_query_param(
        query: str = Form(..., description="The user's query sent to the LLM."),
        index_name: str = Form(..., description="The name of the index to use for the query."),
        filename: Optional[str] = Form(default=None,
                                       description="The name of the file to be used for the query.",
                                       json_schema_extra={"examples": [""]}
                                       ),
        llm_type: str = Form("gpt-3.5-turbo", description=f"The type of language model used. "
                                                            f"Allowed vales are {constants.ALLOWED_LLM_TYPES}"),
        max_tokens: int = Form(100, description="The maximum number of tokens to generate in the response."),
        temperature: float = Form(0.3, description="The sampling temperature to use."),
        top_p: float = Form(0.9, description="The cumulative probability of tokens to consider for the query."),
        disable_public_response: bool = Form(True,
                                             description="If true, the response will "
                                                         "not be fetched from public sources."),
        top_k: int = Form(1, description="The number of top-k results to consider for the query"),
        enable_evaluation: bool = Form(False, description="If true, the answer will "
                                                          "be evaluated using the evaluation metrics."),
        expected_answer: Optional[str] = Form(default=None,
                                              description="The expected answer for the query.",
                                              json_schema_extra={"examples": [""]})
):
    try:
        return QueryParameters(
            query=query,
            index_name=index_name,
            filename=filename,
            llm_type=llm_type,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            disable_public_response=disable_public_response,
            top_k=top_k,
            enable_evaluation=enable_evaluation,
            expected_answer=expected_answer
        )

    except ValidationError as err:
        raise HTTPException(status_code=400, detail=f"{err}") from err


class QueryResponseModel(BaseModel):
    question: str = Field(..., description="The question asked by the user.")
    answer: str = Field(..., description="The answer generated by the LLM.")
    sources: Optional[list] = Field(None, description="List of sources used to generate the answer.")
    evaluation_results: Optional[dict] = Field(None, description="Evaluation results of the answer.")
